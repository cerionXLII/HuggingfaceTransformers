{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Transformers\n",
    "This notebook will use the Hugging Face API to interact with different Transformer achitectures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "#Create instance of the sentiment analysis pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998427629470825},\n",
       " {'label': 'NEGATIVE', 'score': 0.9979380965232849}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = [\"You are such a nice person\", \"You are so lazy, you should really do something good for the world instead\"]\n",
    "classifier(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an instance of the classifier pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'You should do one more repetition, you can do it!',\n",
       " 'labels': ['training', 'education', 'cooking', 'politics'],\n",
       " 'scores': [0.8017725348472595,\n",
       "  0.16816730797290802,\n",
       "  0.02121736854314804,\n",
       "  0.008842849172651768]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = \"You should do one more repetition, you can do it!\"\n",
    "candidate_labels = [\"politics\", \"training\", \"education\", \"cooking\"]\n",
    "\n",
    "classifier(input, candidate_labels=candidate_labels,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       " 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'],\n",
       " 'scores': [0.5036354064941406,\n",
       "  0.4787999987602234,\n",
       "  0.012600099667906761,\n",
       "  0.0026557904202491045,\n",
       "  0.0023087533190846443]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "oracle = pipeline(model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "oracle(\n",
    "    \"I have a problem with my iphone that needs to be resolved asap!!\",\n",
    "    candidate_labels=[\"urgent\", \"not urgent\", \"phone\", \"tablet\", \"computer\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I would like to know the meaning of life. The meaning of life is always an endless life. That is my life. The meaning of life is that in an unlimited and endless life there exists life that can only be achieved through hard work. For every man who will come back at the age of ninety-nine, every man who will come to freedom within twelve months, and every man who would come to everlasting happiness, there will exist life that no man can live without. There is a life that is eternal. That is the life that is eternal. But some must be saved and there isn't one of you who has the ability to live, for there are many who can have an infinite\"}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Hello, I would like to know the meaning of life. The meaning of life is\"\n",
    "generator(prompt, max_length=142, num_return_sequences=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\cerion\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\pipelines\\token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.87922454,\n",
       "  'word': 'Pluto',\n",
       "  'start': 16,\n",
       "  'end': 21},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9412043,\n",
       "  'word': 'Pildammarna',\n",
       "  'start': 56,\n",
       "  'end': 67},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9957456,\n",
       "  'word': 'Malmö',\n",
       "  'start': 71,\n",
       "  'end': 76}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = pipeline(\"ner\", grouped_entities=True)\n",
    "text = 'My dog is named Pluto, and he wants to play in the park Pildammarna in Malmö'\n",
    "\n",
    "model(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question and answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.4931827187538147,\n",
       " 'start': 20,\n",
       " 'end': 33,\n",
       " 'answer': 'ten years old'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = pipeline(\"question-answering\")\n",
    "context = 'I have a dog who is ten years old. I got my kitten when my dog was two. They like to play together.'\n",
    "question = 'How old is my cat?'\n",
    "model(question=question, context=context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ich liebe den Sommer, es riecht so wunderbar!\n"
     ]
    }
   ],
   "source": [
    "#text = \"The house is wonderful.\"\n",
    "text = \"I love the summer, it smells so wonderful!\"\n",
    "input_ids = tokenizer(f'translate English to German: {text}', return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids, max_new_tokens=142)\n",
    "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ich liebe den Sommer, es riecht so wunderbar!!\n"
     ]
    }
   ],
   "source": [
    "text = \"Ich liebe den Sommer, es riecht so wunderbar!!\"\n",
    "input_ids = tokenizer(f'Translate to English: {text}', return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids, max_new_tokens=142)\n",
    "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behind the pipeline\n",
    "The pipeline does a lot of stuff for us:\n",
    "- Preprocessing inputs\n",
    "- Running the model\n",
    "- Postprocessing output\n",
    "\n",
    "Let's try and do the same step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998676776885986},\n",
       " {'label': 'NEGATIVE', 'score': 0.9972905516624451}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "raw_inputs =  [\n",
    "        \"I just want to be happy.\",\n",
    "        \"I hate sallad\",\n",
    "    ]\n",
    "\n",
    "#Using the pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "classifier(raw_inputs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  2074,  2215,  2000,  2022,  3407,  1012,   102],\n",
      "        [  101,  1045,  5223, 16183, 27266,   102,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# Using the model directly and performing the steps manually\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'just', 'want', 'to', 'be', 'happy', '.', 'i', 'hate', 'sal', '##lad']\n"
     ]
    }
   ],
   "source": [
    "#Look at the raw tokens\n",
    "input_tokens = tokenizer.tokenize(raw_inputs)\n",
    "print(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the base model\n",
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 9, 768])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the model for sequence classification\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "#Get the outputs as logits\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-4.2892,  4.6415],\n",
       "        [ 3.1981, -2.7102]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(outputs.logits.shape)\n",
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.3226e-04, 9.9987e-01],\n",
       "        [9.9729e-01, 2.7094e-03]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalize them to probabilities\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "probs = F.softmax(outputs.logits, dim=-1)\n",
    "print(probs.shape)\n",
    "\n",
    "#Should be the same as the pipeline\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I just want to be happy.\n",
      "Predicted label: POSITIVE\n",
      "Predicted probability: 0.9998676776885986\n",
      "\n",
      "Input: I hate sallad\n",
      "Predicted label: NEGATIVE\n",
      "Predicted probability: 0.9972905516624451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ids = np.array(torch.argmax(probs, dim=-1) )\n",
    "for i, id in enumerate(ids):\n",
    "    print(f\"Input: {raw_inputs[i]}\")\n",
    "    print(f\"Predicted label: {model.config.id2label[id]}\")\n",
    "    print(f\"Predicted probability: {probs[i][id]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create transformer models from a blue print\n",
    "The config contain all that is needed to create a model like BERT, BART, GPT etc\n",
    "We can initialize any model randomly using the config.\n",
    "Then we can download a checkpoint to fill in the model weights, or train on our own like any PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.36.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "<class 'transformers.models.bert.modeling_bert.BertModel'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config (random initialization)\n",
    "model = BertModel(config)\n",
    "\n",
    "print(config)\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../models/bert-base-uncased_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
